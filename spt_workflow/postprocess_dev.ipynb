{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "postprocess_flow_forecasts.py\n",
    "\n",
    "Author: Riley Hales\n",
    "Copyright March 2020\n",
    "License: BSD 3 Clause\n",
    "\n",
    "Identifies flows forecasted to experience a return period level flow on streams from a preprocessed list of stream\n",
    "COMID's in each region\n",
    "\"\"\"\n",
    "import datetime\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_forecast_qout_files(rapidio_region_output):\n",
    "    # pick the most recent date, append to the file path\n",
    "    recent_date = sorted(os.listdir(rapidio_region_output))\n",
    "    recent_date = recent_date[-1]\n",
    "    qout_folder = os.path.join(rapidio_region_output, recent_date)\n",
    "    # list the forecast files\n",
    "    prediction_files = sorted(glob.glob(os.path.join(qout_folder, 'Qout*.nc')))\n",
    "\n",
    "    # merge them into a single file joined by ensemble number\n",
    "    ensemble_index_list = []\n",
    "    qout_datasets = []\n",
    "    for forecast_nc in prediction_files:\n",
    "        ensemble_index_list.append(int(os.path.basename(forecast_nc)[:-3].split(\"_\")[-1]))\n",
    "        qout_datasets.append(xarray.open_dataset(forecast_nc).Qout)\n",
    "    return xarray.concat(qout_datasets, pd.Index(ensemble_index_list, name='ensemble')), qout_folder\n",
    "\n",
    "\n",
    "def check_for_return_period_flow(largeflows_df, forecasted_flows_df, stream_order, rp_data):\n",
    "    max_flow = max(forecasted_flows_df['means'])\n",
    "\n",
    "    # temporary dates\n",
    "    date_r2 = ''\n",
    "    date_r10 = ''\n",
    "    date_r20 = ''\n",
    "    date_r25 = ''\n",
    "    date_r50 = ''\n",
    "    date_r100 = ''\n",
    "\n",
    "    # retrieve return period flow levels from dataframe\n",
    "    r2 = float(rp_data['return_period_2'].values[0])\n",
    "    r10 = float(rp_data['return_period_10'].values[0])\n",
    "    r20 = float(rp_data['return_period_20'].values[0])\n",
    "    # r25 = float(rp_data['return_period_25'].values[0])\n",
    "    # r50 = float(rp_data['return_period_50'].values[0])\n",
    "    # r100 = float(rp_data['return_period_100'].values[0])\n",
    "\n",
    "    # then compare the timeseries to the return period thresholds\n",
    "    if max_flow >= r2:\n",
    "        date_r2 = get_time_of_first_exceedence(forecasted_flows_df, r2)\n",
    "    # if the flow is not larger than the smallest return period, return the dataframe without appending anything\n",
    "    else:\n",
    "        return largeflows_df\n",
    "\n",
    "    # check the rest of the return period flow levels\n",
    "    if max_flow >= r10:\n",
    "        date_r10 = get_time_of_first_exceedence(forecasted_flows_df, r10)\n",
    "    if max_flow >= r20:\n",
    "        date_r20 = get_time_of_first_exceedence(forecasted_flows_df, r20)\n",
    "    # if max_flow >= r25:\n",
    "    #     date_r25 = get_time_of_first_exceedence(forecasted_flows_df, r25)\n",
    "    # if max_flow >= r50:\n",
    "    #     date_r50 = get_time_of_first_exceedence(forecasted_flows_df, r50)\n",
    "    # if max_flow >= r100:\n",
    "    #     date_r100 = get_time_of_first_exceedence(forecasted_flows_df, r100)\n",
    "\n",
    "    return largeflows_df.append({\n",
    "        'comid': rp_data.index,\n",
    "        'stream_order': stream_order,\n",
    "        'stream_lat': rp_data['lat'],\n",
    "        'stream_lon': rp_data['lon'],\n",
    "        'max_flow': max_flow,\n",
    "        'date_r2': date_r2,\n",
    "        'date_r10': date_r10,\n",
    "        'date_r20': date_r20,\n",
    "        # 'date_r25': date_r25,\n",
    "        # 'date_r50': date_r50,\n",
    "        # 'date_r100': date_r100,\n",
    "    }, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_time_of_first_exceedence(forecasted_flows_df, flow):\n",
    "    # replace the flows that are too small (don't exceed the return period)\n",
    "    forecasted_flows_df[forecasted_flows_df.means < flow] = np.nan\n",
    "    daily_flows = forecasted_flows_df.dropna()\n",
    "    return daily_flows['times'].values[0]\n",
    "\n",
    "\n",
    "def find_forecast_record_netcdf(region, forecast_records, qout_folder, year):\n",
    "    record_path = os.path.join(forecast_records, region, 'forecast_record-' + year + '-' + region + '.nc')\n",
    "    # if there isn't a forecast record for this year\n",
    "    if not os.path.exists(record_path):\n",
    "        # using a forecast file as a reference\n",
    "        reference = glob.glob(os.path.join(qout_folder, 'Qout*.nc'))[0]\n",
    "        reference = nc.Dataset(reference)\n",
    "        # make a new record file\n",
    "        record = nc.Dataset(record_path, 'w')\n",
    "        # copy the right dimensions and variables\n",
    "        record.createDimension('time', None)\n",
    "        record.createDimension('rivid', reference.dimensions['rivid'].size)\n",
    "        record.createVariable('time', reference.variables['time'].dtype, dimensions=('time',))\n",
    "        record.createVariable('lat', reference.variables['lat'].dtype, dimensions=('rivid',))\n",
    "        record.createVariable('lon', reference.variables['lon'].dtype, dimensions=('rivid',))\n",
    "        record.createVariable('rivid', reference.variables['rivid'].dtype, dimensions=('rivid',))\n",
    "        record.createVariable('Qout', reference.variables['Qout'].dtype, dimensions=('rivid', 'time'))\n",
    "        # and also prepopulate the lat, lon, and rivid fields\n",
    "        record.variables['rivid'][:] = reference.variables['rivid'][:]\n",
    "        record.variables['lat'][:] = reference.variables['lat'][:]\n",
    "        record.variables['lon'][:] = reference.variables['lon'][:]\n",
    "        # calculate the time variable's steps 'hours since YYYY0101 00:00:00' hours since midnight on new years day\n",
    "        date = datetime.datetime(year=int(year), month=1, day=1, hour=0, minute=0, second=0)\n",
    "        end = int(year) + 1\n",
    "        forecast_timesteps = []\n",
    "        while date.year < end:\n",
    "            forecast_timesteps.append(datetime.datetime.timestamp(date))\n",
    "            date = date + datetime.timedelta(hours=3)\n",
    "        record.variables['time'][:] = forecast_timesteps\n",
    "        record.close()\n",
    "    \n",
    "    return nc.Dataset(record_path, 'a')\n",
    "\n",
    "\n",
    "def add_to_forecast_record(comid, forecast_record_file, first_day_flow_df):\n",
    "    comid_index = list(forecast_record_file.variables['rivid'][:]).index(comid)\n",
    "    record_times = list(forecast_record_file.variables['time'][:])\n",
    "    day_times = first_day_flow_df['times']\n",
    "    day_flows = first_day_flow_df['means']\n",
    "    \n",
    "    for time, flow in zip(day_times, day_flows):\n",
    "        idx = record_times.index(datetime.datetime.timestamp(time))\n",
    "        forecast_record_file.variables['Qout'][comid_index, idx] = flow\n",
    "    return forecast_record_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapidio = '/Users/rileyhales/SpatialData/SPT/rapid-io/'\n",
    "historical_sim = '/Users/rileyhales/SpatialData/SPT/historical/'\n",
    "logs_dir = '/Users/rileyhales/SpatialData/SPT/logs/'\n",
    "forecast_records = '/Users/rileyhales/SpatialData/SPT/forecastrecords/'\n",
    "\n",
    "# list of regions to be processed based on their forecasts\n",
    "regions = os.listdir(os.path.join(rapidio, 'input'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'islands-geoglows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the propert directory paths\n",
    "rapidio_region_input = os.path.join(rapidio, 'input', region)\n",
    "rapidio_region_output = os.path.join(rapidio, 'output', region)\n",
    "\n",
    "# make the pandas dataframe to store the summary info\n",
    "largeflows = pd.DataFrame(columns=[\n",
    "    'comid', 'stream_order', 'stream_lat', 'stream_lon', 'max_flow', 'date_r2', 'date_r10', 'date_r20'])\n",
    "# , 'date_r25', 'date_r50', 'date_r100'])\n",
    "\n",
    "# merge the most recent forecast files into a single xarray dataset\n",
    "merged_forecasts, qout_folder = merge_forecast_qout_files(rapidio_region_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the times and comids from the forecasts\n",
    "times = pd.to_datetime(pd.Series(merged_forecasts.time))\n",
    "comids = pd.Series(merged_forecasts.rivid)\n",
    "tomorrow = times[0] + pd.Timedelta(days=1)\n",
    "year = times[0].strftime(\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the return period file\n",
    "return_period_file = glob.glob(os.path.join(historical_sim, region, 'return_period*.nc'))[0]\n",
    "return_period_data = xarray.open_dataset(return_period_file).to_dataframe()\n",
    "\n",
    "# read the list of large streams\n",
    "stream_list = os.path.join(rapidio_region_input, 'large_str-' + region + '.csv')\n",
    "large_streams_df = pd.read_csv(stream_list)\n",
    "large_list = large_streams_df['COMID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the netcdf used to store the forecast record\n",
    "forecast_record_file = find_forecast_record_netcdf(region, forecast_records, qout_folder, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now process the mean flows for each river in the region\n",
    "for comid in comids:\n",
    "    # compute the timeseries of average flows\n",
    "    means = np.array(merged_forecasts.sel(rivid=comid)).mean(axis=0)\n",
    "    # put it in a dataframe with the times series\n",
    "    forecasted_flows = times.to_frame(name='times').join(pd.Series(means, name='means')).dropna()\n",
    "    # select flows in 1st day and save them to the forecast record\n",
    "    forecast_record_file = add_to_forecast_record(comid, forecast_record_file, forecasted_flows[forecasted_flows.times < tomorrow])\n",
    "    \n",
    "    # if stream order is larger than 2, check if it needs to be included on the return periods summary csv\n",
    "    if comid in large_list:\n",
    "        max_flow = max(means)\n",
    "        order = large_streams_df[large_streams_df.COMID == comid]['order_']\n",
    "        rp_data = return_period_data[return_period_data.index == comid]\n",
    "        largeflows = check_for_return_period_flow(largeflows, forecasted_flows, order, rp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comid</th>\n",
       "      <th>stream_order</th>\n",
       "      <th>stream_lat</th>\n",
       "      <th>stream_lon</th>\n",
       "      <th>max_flow</th>\n",
       "      <th>date_r2</th>\n",
       "      <th>date_r10</th>\n",
       "      <th>date_r20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [comid, stream_order, stream_lat, stream_lon, max_flow, date_r2, date_r10, date_r20]\n",
       "Index: []"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largeflows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeflows.to_csv(os.path.join(qout_folder, 'forecasted_return_periods_summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
